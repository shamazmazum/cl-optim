@begin[ref=index](section)
   @title(Manual)
   CL-OPTIM system provides a collection of algorithms for minimizing real
   functions of one or many arguments.

   @begin(section)
      @title(Gradient-based algorithms)
      These algorithms require the function to be differentiable at almost every
      point. There is no need to define the derivative of the function you wish
      to minimize. A special system,
      @link[uri="https://github.com/shamazmazum/cl-forward-diff"](cl-forward-diff),
      is capable of automatic differentiation. For example, let's write a
      function which evaluates a polynomial at point @c(x) given the polynomial
      coefficients @c(coeffs). For the sake of automatic differentiation we must
      not use standard mathematical functions from the @c(cl) package. Instead
      we must use functions from @c(cl-forward-diff) package. If you mix them
      you will get either runtime error or compile time error (depending on your
      implementation). You will never get a wrong result, so don't worry about
      accidentally mixing these functions. To minimize a probability of an error
      I recommend to define differentiable functions in a separate package which
      shadows mathematical functions from @c(cl) with ones provided by
      @c(cl-forward-diff). Our function, @c(polynomial) can look like this:
      @begin[lang=lisp](code)
(defpackage polynomial
  (:use #:cl #:snakes)
  (:import-from #:cl-forward-diff #:dual)
  (:import-from #:serapeum #:->)
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:polynomial))
(in-package :polynomial)

(-> polynomial (list dual) (values dual &optional))
(defun polynomial (coeffs x)
  (declare (type dual x))
  (reduce-generator
   #'+
   (imap (lambda (c n)
           (* c (expt x n)))
         (list->generator coeffs)
         (icount 0))))
      @end(code)
      Type declarations are added for clarity, but combining them with
      declarations like @c((declare (optimize (speed 3)))) can also result in
      better generated code.

      As you can see, differentiable functions operate not with usual numbers
      but with values of type @c(dual) (so called dual numbers). You can
      evaluate some polynomial, let it be \(f(x) = 1 + 2x + 3x^2\) at the
      point \(x\) (let \(x = 2\)) along with its first derivative \(f'(x) = 2 +
      6x\) like this:
      @begin[lang=repl](code)
CL-USER> (polynomial:polynomial '(1 2 3) #d(2d0 1d0))
#<SIMD-PACK 1.7000000000000d+1 1.4000000000000d+1>
      @end(code)
      The type @c(dual) is implemented as a structure which has two slots for
      double float numbers, so all calculations are performed with double
      precision.

      Now when we have this function, @c(polynomial), we want to find the
      minimum of our polynomial f(x). We can use the simplest gradient descent
      algorithm for this purpose. Try the following code in the REPL:
      @begin[lang=repl](code)
CL-USER> (cl-optim:gradient-descent
 (alexandria:compose (alexandria:curry #'polynomial:polynomial '(1 2 3))
                     (alexandria:rcurry #'aref 0))
 (cl-forward-diff:to-doubles '(10)))
#(-0.3331667786236352d0)
3673
      @end(code)
      The first argument of @c(gradient-descent) is a function to be
      minimized. It takes a vector of dual numbers (let its length be @c(n)) and
      return a dual number. We must compose partially applied @c(polynomial)
      with @c((lambda (xs) (aref xs 0))) because @c(polynomial) excepts a single
      dual number, not a vector of duals. The second argument is a starting
      point for a search. It is a vector of double floats of the same length
      @c(n). Because our function is convex, just every starting point will do.

      The first returned value is the found minimum and the second is the number
      of steps required to find that minimum. You can use a better algorithm to
      reduce the number of steps:
      @begin[lang=repl](code)
CL-USER> (cl-optim:nag
          (alexandria:compose (alexandria:curry #'polynomial:polynomial '(1 2 3))
                     (alexandria:rcurry #'aref 0))
          '(10))
#(-0.33344855804382767d0)
139
      @end(code)

      Just another example of minimizing a function of two variables (Rosenbrock
      function with parameters @c(a = 2) and @c(b = 100)).
      @begin[lang=lisp](code)
(defpackage rosenbrock
  (:use #:cl)
  (:import-from #:cl-forward-diff #:dual)
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:rosenbrock))
(in-package :rosenbrock)

(defun rosenbrock (xs)
  (let ((x (aref xs 0))
        (y (aref xs 1)))
    (+ (expt (- 2 x) 2)
       (* 100 (expt (- y (expt x 2)) 2)))))
      @end(code)
      Evaluate in the REPL:
      @begin[lang=repl](code)
CL-USER> (cl-optim:nag #'rosenbrock:rosenbrock
                       (cl-forward-diff:to-doubles '(-1 1))
                       :η  1d-4
                       :ε  1d-5
                       :β1 0.99d0)
#(1.999993247190274d0 3.9999729695129296d0)
5275
      @end(code)
      Here I overrided some default parameters @c(β1), @c(ε) and @c(η). For more
      information see @ref[id=api](API) section.
   @end(section)

   @begin(section)
      @title(Algorithms which work without gradient)

      Currently there is only one algorithm in @c(cl-optim) which can optimize
      non-differentiable functions: simulated annealing. The function you wish
      to minimize must operate with @c(double-float) numbers, not with
      @c(dual)s. Take this function as an example.

      @begin[lang=lisp](code)
(defun noise-sinc (x)
  (declare (type double-float x))
  ;; Here all math function are from CL package.
  (let ((x-shift (- x 4)))
    (- (random 2d-1)
       (if (zerop x-shift) 1d0
           (/ (sin (* 5 x-shift)) x-shift)))))
      @end(code)

      This is a @i(sinc) function subtracted from a random noise with a small
      amplitude. This function can not be differentiated due to this noise. Now
      try this in the REPL:

      @begin[lang=repl](code)
CL-USER> (cl-optim:simulated-annealing
          (alexandria:compose #'noise-sinc
                              (alexandria:rcurry #'aref 0))
          (cl-forward-diff:to-doubles '(15)))
#(4.0656966038580755d0)
#S(CL-OPTIM:SIMULATED-ANNEALING-SUMMARY
   :TEMPERATURE 9.99734336301017d-5
   :ITERATIONS 27764
   :MINIMIZING-STEPS 8247
   :REJECTED-STEPS 18558)
      @end(code)

      The minimum is around the point @c((4d0)). As you can see, this method
      does a big amount of evaluations of the function, but it does not require
      the function to be differentiable.
   @end(section)

   @begin(section)
   @title(Linear least squares)
   You can solve linear least squares fit problem with @c(cl-optim). Suppose you
   have a vector of arguments of some function (x'es) and function of values
   (y's). You can approximate that function with a linear function like so:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 3))
          (cl-forward-diff:to-doubles '(1 2 4)))
#(0.6428571428571426d0 0.4999999999999998d0)
0.07142857142857147d0
@end(code)

   Your linear approximation is \(f(x) = 0.64 x + 0.5\). The second value is
   the sum of squared differences between \(f(x)\) and supplied \(y\). When
   a number of samples is 2, the second value is zero:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 3))
          (cl-forward-diff:to-doubles '(1 5)))
#(0.5d0 0.5d0)
0.0d0
@end(code)
   Indeed, \(1 \cdot \frac{1}{2} + \frac{1}{2} = 1\) and \(5 \cdot \frac{1}{2} +
   \frac{1}{2} = 3\).

   Multivariate problem can also be solved:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 6))
          (cl-forward-diff:to-doubles '(1 2 2))
          (cl-forward-diff:to-doubles '(1 2 1)))
#(4.9999999999999964d0 -3.999999999999991d0 -1.0658141036401503d-14)
1.072850831100576d-28
@end(code)
   which corresponds to the function \(f(x_0, x_1) = 5 x_0 - 4 x_1\). Indeed, we
   have \(5 \cdot 1 - 4 \cdot 1 = 1\), \(5 \cdot 2 - 4 \cdot 2 = 2\) and \(5
   \cdot 2 - 4 \cdot 1 = 6\).
   @end(section)
@end(section)

@begin[ref=api](section)
   @title(API)
   @u(Gradient-based algorithms)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function gradient-descent)
      @cl:doc(function gradient-descent-momentum)
      @cl:doc(function nag)
      @cl:doc(function adam)
   )
   @u(Gradient-based algorithms with Hessian approximation)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function bfgs)
      @cl:doc(function bfgs/magicl)
      @cl:doc(function backtracking-options)
   )
   @u(Least squares fit)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function linear-least-squares)
   )
   @u(Heuristics)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function simulated-annealing)
      @cl:doc(function exponential-cooldown)
      @cl:doc(function normal-neighborhood)
   )
   @u(Parameters)
   @cl:with-package[name="cl-optim"](
      @cl:doc(variable *ε*)
      @cl:doc(variable *η*)
      @cl:doc(variable *β1*)
      @cl:doc(variable *β2*)
      @cl:doc(variable *max-iterations*)
      @cl:doc(variable *final-temperature*)
      @cl:doc(variable *default-backtracking-options*)
   )
@end(section)

@begin(section)
   @title(Tips)
   @begin(list)
   @item(Do not forgot to use @c(optimize (speed 3)) declaration in functions
   which may be a bottleneck. Though not as smart as with ordinary math
   functions, SBCL can produce nice assembly code when optimizing for speed.)
   @item(If you use Emacs with julia-mode, the lines at the end of this section
   added to @c(~/.emacs) will help you to type Greek symbols. For example, to
   type ε you need to type @c(\upepsilon) and press Tab.)
   @end(list)

   @begin[lang=lisp](code)
(defun indent-or-latexsub (arg)
  (interactive "*i")
  (if (latexsub)
      (indent-for-tab-command)))

(global-set-key (kbd "TAB") 'indent-or-latexsub)
   @end(code)
@end(section)

@begin(section)
   @title(More examples)

   @begin(section)
      @title(Solving a system of linear equations)

      Consider this system of linear equations:
      @begin(code)
 x + 2y +  z + 3w = 18
2x +  y +  z +  w = 12
 x + 3y + 2z + 4w = 28
 x + 5y + z  +  w = 23
      @end(code)
      or \(Au = v\) in the matrix form where \(v\) is a column containing the
      right hand of the system and \(A\) is a 4x4 matrix containing coefficients
      on the left. The solution \(u\) minimizes the cost function \(\sum_k (Au -
      v_k)^2\). The code for the cost function is the following (requires
      @c(array-operations) library):
      @begin[lang=lisp](code)
(defpackage linear-system
  (:use #:cl)
  (:local-nicknames (#:sera #:serapeum)
                    (#:diff #:cl-forward-diff))
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:linear-system-cost #:%make-array))
(in-package :linear-system)

(sera:-> matrix-vector-mul
         ((simple-array diff:dual (cl:* cl:*))
          (simple-array diff:dual (cl:*)))
         (values (simple-array diff:dual (cl:*)) &optional))
(defun matrix-vector-mul (mat vec)
  "Multiply a matrix MAT by a column VEC"
  (aops:each-index* 'diff:dual i
      (aops:reduce-index #'+ j
          (* (aref mat i j) (aref vec j)))))

(sera:-> linear-system-cost
         ((simple-array diff:dual (cl:* cl:*))
          (simple-array diff:dual (cl:*))
          (simple-array diff:dual (cl:*)))
         (values diff:dual &optional))
(defun linear-system-cost (mat v u)
  "Calculate a cost function for a system of linear equations MAT*X=V at the
  point X=U"
  (let ((val (matrix-vector-mul mat u)))
    (reduce
     #'+ (map '(vector diff:dual)
              (lambda (v1 v2)
                (expt (- v1 v2) 2))
              v val))))

(defun %make-array (list shape)
  "Helper function to convert lists to arrays of dual numbers"
  (labels ((to-dual (list)
             (mapcar (lambda (x)
                       (if (listp x) (to-dual x)
                           (diff:make-dual (float x 0d0))))
                     list)))
    (make-array shape
                :element-type     'diff:dual
                :initial-contents (to-dual list))))
      @end(code)

      Now evaluate the following code in the REPL:
      @begin[lang=lisp](code)
(cl-optim:bfgs/magicl
 (alexandria:curry #'linear-system:linear-system-cost
                   (linear-system:%make-array
                    '((1 2 1 3)
                      (2 1 1 1)
                      (1 3 2 4)
                      (1 5 1 1))
                    '(4 4))
                   (linear-system:%make-array
                    '(18 12 28 23)
                    '(4)))
 (magicl:zeros '(4)))
      @end(code)

      The answer is
      @begin[lang=lisp](code)
#<MAGICL:VECTOR/DOUBLE-FLOAT (4):
   1.000
   3.000
   5.000
   2.000>
#<MAGICL:MATRIX/DOUBLE-FLOAT (4x4):
  14.000    24.000    12.000    20.000
  24.000    77.993    28.000    48.002
  12.000    28.000    14.000    26.000
  20.000    48.002    26.000    53.998>
7
      @end(code)
      The first value is the minimizer of @c(linear-system-cost): \((1, 3, 5,
      2)\). The second value is Hessian of the cost function at \(x\). The last
      value is a number of iterations needed to minimize the cost function.
   @end(section)
@end(section)
