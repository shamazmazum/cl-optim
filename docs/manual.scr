@begin[ref=index](section)
   @title(Manual)
   CL-OPTIM system provides a collection of algorithms for minimizing real
   functions of one or many arguments.

   Because Codex (the system with which this documentation is generated) cannot
   add type annotations, it's important to mention that most of the algos in
   this library will require a @i(gradient function) which is a function which
   takes a simple array of double floats (arguments) and returns a simple array
   of double floats (gradient values at this point) and/or a @i(target function)
   which is to be minimized which takes a simple array of double floats and
   returns a double float (its value at this point).

   @begin(section)
      @title(Gradient-based algorithms)
      These algorithms require a gradient function to be defined along with a
      target function. A gradient function can be obtained automatically with
      the help of my another library,
      @link[uri="https://github.com/shamazmazum/cl-forward-diff"](cl-forward-diff),
      but you can define it manually.

      For example, let's write a function which evaluates a polynomial at point
      @c(x) given the polynomial coefficients @c(coeffs) and another function
      which returns the gradient of the first function. For the sake of
      automatic differentiation we must not use standard mathematical functions
      from the @c(cl) package. Instead we must use functions from
      @c(cl-forward-diff) package. If you mix them you will get either runtime
      error or compile time error (depending on your implementation). You will
      never get a wrong result, so don't worry about accidentally mixing these
      functions. To minimize a probability of an error I recommend to define
      differentiable functions in a separate package which shadows mathematical
      functions from @c(cl) with ones provided by @c(cl-forward-diff). Our
      functions, @c(polynomial) and @c(polynomial-grad) can look like this:
      @begin[lang=lisp](code)
(defpackage polynomial
  (:use #:cl)
  (:local-nicknames (#:si   #:stateless-iterators)
                    (#:diff #:cl-forward-diff))
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:polynomial #:polynomial-gradient))
(in-package :polynomial)

(defun polynomial (coeffs x)
  ;; Extract the first argument from the array of arguments
  (let ((x (aref x 0)))
    ;; Evaluate the polynomial at the point X
    (si:foldl
     #'+ 0
     (si:imap (lambda (c n)
                (* c (expt x n)))
              (si:list->iterator coeffs)
              (si:count-from 0)))))

(defun polynomial-gradient (coeffs x)
  ;; Use automatic differentiation to extract the value of derivative
  ;; at the point X
  (diff:ad-multivariate
   (lambda (x) (polynomial coeffs x))
   x))
   @end(code)

      You can evaluate a polynomial, let it be \(f(x) = 1 + 2x + 3x^2\) at the
      point \(x\) (let \(x = 2\)) and its first derivative \(f'(x) = 2 + 6x\)
      like this (@c(to-doubles) is a helper which generates a simple array of
      double floats from a sequence of reals):
      @begin[lang=lisp](code)
CL-USER> (polynomial:polynomial '(1 2 3) (cl-forward-diff:to-doubles '(2)))
17.0d0
CL-USER> (polynomial:polynomial-gradient '(1 2 3) (cl-forward-diff:to-doubles '(2)))
#(14.0d0)
      @end(code)

      Now when we have the gradient function, @c(polynomial-gradient), we want
      to find the minimum of our polynomial. We can use the simplest gradient
      descent algorithm for this purpose. Try the following code in the REPL:
      @begin[lang=lisp](code)
CL-USER> (cl-optim:gradient-descent
 (lambda (x) (polynomial:polynomial-gradient '(1 2 3) x))
 (cl-forward-diff:to-doubles '(1)))
#(-0.3331670485897465d0)
2992
      @end(code)
      The first argument of @c(gradient-descent) is the gradient of a function
      to be minimized. The second argument is a starting point for a
      search. Because our function is convex, just every starting point will do.

      The first returned value is the found minimum and the second is the number
      of steps required to find that minimum. You can use a better algorithm to
      reduce the number of steps:
      @begin[lang=lisp](code)
CL-USER> (cl-optim:nag
 (lambda (x) (polynomial:polynomial-gradient '(1 2 3) x))
 (cl-forward-diff:to-doubles '(1)))
#(-0.3331680985581683d0)
136
      @end(code)

      Just another example of minimizing a function of two variables (Rosenbrock
      function with parameters @c(a = 2) and @c(b = 100)).
      @begin[lang=lisp](code)
(defpackage rosenbrock
  (:use #:cl)
  (:local-nicknames (#:diff #:cl-forward-diff))
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:rosenbrock))
(in-package :rosenbrock)

(defun rosenbrock (xs)
  (let ((x (aref xs 0))
        (y (aref xs 1)))
    (+ (expt (- 2 x) 2)
       (* 100 (expt (- y (expt x 2)) 2)))))

(defun rosenbrock-gradient (x)
  (diff:ad-multivariate #'rosenbrock x))
      @end(code)
      Evaluate in the REPL:
      @begin[lang=lisp](code)
CL-USER> (cl-optim:nag #'rosenbrock:rosenbrock
                       (cl-forward-diff:to-doubles '(-1 1))
                       :η  1d-4
                       :ε  1d-5
                       :β1 0.99d0)
#(1.9999787688313526d0 3.9999150258343437d0)
6693
      @end(code)
      Here I overrided some default parameters @c(β1), @c(ε) and @c(η). For more
      information see @ref[id=api](API) section.

      The best algorithm is BFGS:
      @begin[lang=lisp](code)
      CL-USER> (cl-optim:bfgs #'rosenbrock:rosenbrock #'rosenbrock:rosenbrock-gradient
                       (cl-forward-diff:to-doubles '(-1 1)))
#(2.000000000000671d0 4.000000000002626d0)
#<MAGICL:MATRIX/DOUBLE-FLOAT (2x2):
  3192.251    -797.639
  -797.639    199.428>
40
      @end(code)
   @end(section)

   @begin(section)
      @title(Algorithms which work without gradient)

      Currently there is only one algorithm in @c(cl-optim) which can optimize
      non-differentiable functions: simulated annealing. Let's minimize the
      following function:

      @begin[lang=lisp](code)
(defun noise-sinc (x)
  (declare (type double-float x))
  ;; Here all math function are from CL package.
  (let ((x-shift (- x 4)))
    (- (random 2d-1)
       (if (zerop x-shift) 1d0
           (/ (sin (* 5 x-shift)) x-shift)))))
      @end(code)

      This is a @i(sinc) function subtracted from a random noise with a small
      amplitude. This function can not be differentiated due to this noise. Now
      try this in the REPL:

      @begin[lang=lisp](code)
CL-USER> (cl-optim:simulated-annealing
          (alexandria:compose #'noise-sinc
                              (alexandria:rcurry #'aref 0))
          (cl-forward-diff:to-doubles '(15)))
#(4.0656966038580755d0)
#S(CL-OPTIM:SIMULATED-ANNEALING-SUMMARY
   :TEMPERATURE 9.99734336301017d-5
   :ITERATIONS 27764
   :MINIMIZING-STEPS 8247
   :REJECTED-STEPS 18558)
      @end(code)

      The minimum is around the point @c((4d0)). As you can see, this method
      does a big amount of evaluations of the function, but it does not require
      the function to be differentiable.
   @end(section)

   @begin(section)
   @title(Linear least squares)
   You can solve linear least squares fit problem with @c(cl-optim). Suppose you
   have a vector of arguments of some function (x'es) and function of values
   (y's). You can approximate that function with a linear function like so:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 3))
          (cl-forward-diff:to-doubles '(1 2 4)))
#(0.6428571428571426d0 0.4999999999999998d0)
0.07142857142857147d0
@end(code)

   Your linear approximation is \(f(x) = 0.64 x + 0.5\). The second value is
   the sum of squared differences between \(f(x)\) and supplied \(y\). When
   a number of samples is 2, the second value is zero:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 3))
          (cl-forward-diff:to-doubles '(1 5)))
#(0.5d0 0.5d0)
0.0d0
@end(code)
   Indeed, \(1 \cdot \frac{1}{2} + \frac{1}{2} = 1\) and \(5 \cdot \frac{1}{2} +
   \frac{1}{2} = 3\).

   Multivariate problem can also be solved:
@begin[lang=lisp](code)
CL-USER> (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 6))
          (cl-forward-diff:to-doubles '(1 2 2))
          (cl-forward-diff:to-doubles '(1 2 1)))
#(4.9999999999999964d0 -3.999999999999991d0 -1.0658141036401503d-14)
1.072850831100576d-28
@end(code)
   which corresponds to the function \(f(x_0, x_1) = 5 x_0 - 4 x_1\). Indeed, we
   have \(5 \cdot 1 - 4 \cdot 1 = 1\), \(5 \cdot 2 - 4 \cdot 2 = 2\) and \(5
   \cdot 2 - 4 \cdot 1 = 6\).
   @end(section)
@end(section)

@begin[ref=api](section)
   @title(API)
   @u(Gradient-based algorithms)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function gradient-descent)
      @cl:doc(function gradient-descent-momentum)
      @cl:doc(function nag)
      @cl:doc(function adam)
   )
   @u(Gradient-based algorithms with Hessian approximation)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function bfgs)
      @cl:doc(function bfgs/magicl)
      @cl:doc(function backtracking-options)
   )
   @u(Least squares fit)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function linear-least-squares)
      @cl:doc(function linear-irls)
      @cl:doc(variable *default-linear-irls-options*)
   )
   @u(Heuristics)
   @cl:with-package[name="cl-optim"](
      @cl:doc(function simulated-annealing)
      @cl:doc(function exponential-cooldown)
      @cl:doc(function normal-neighborhood)
   )
   @u(Parameters)
   @cl:with-package[name="cl-optim"](
      @cl:doc(variable *ε*)
      @cl:doc(variable *η*)
      @cl:doc(variable *β1*)
      @cl:doc(variable *β2*)
      @cl:doc(variable *max-iterations*)
      @cl:doc(variable *final-temperature*)
      @cl:doc(variable *default-backtracking-options*)
   )
@end(section)

@begin(section)
   @title(Tips)
   @begin(list)
   @item(Do not forgot to use @c(optimize (speed 3)) declaration in functions
   which may be a bottleneck. Though not as smart as with ordinary math
   functions, SBCL can produce nice assembly code when optimizing for speed.)
   @item(If you use Emacs with julia-mode, the lines at the end of this section
   added to @c(~/.emacs) will help you to type Greek symbols. For example, to
   type ε you need to type @c(\upepsilon) and press Tab.)
   @end(list)

   @begin[lang=lisp](code)
(defun indent-or-latexsub (arg)
  (interactive "*i")
  (if (latexsub)
      (indent-for-tab-command)))

(global-set-key (kbd "TAB") 'indent-or-latexsub)
   @end(code)
@end(section)

@begin(section)
   @title(More examples)

   @begin(section)
      @title(Solving a system of linear equations)

      Consider this system of linear equations:
      @begin(code)
 x + 2y +  z + 3w = 18
2x +  y +  z +  w = 12
 x + 3y + 2z + 4w = 28
 x + 5y + z  +  w = 23
      @end(code)
      or \(Au = v\) in the matrix form where \(v\) is a column containing the
      right hand of the system and \(A\) is a 4x4 matrix containing coefficients
      on the left. The solution \(u\) minimizes the cost function \(\sum_k (Au -
      v_k)^2\). The code for the cost function is the following (requires
      @c(array-operations) library):
      @begin[lang=lisp](code)
(defpackage linear-system
  (:use #:cl)
  #.(cl-forward-diff:shadowing-import-math)
  (:local-nicknames (#:diff #:cl-forward-diff))
  (:export #:my-system #:my-system-grad))
(in-package :linear-system)

(defun matrix-vector-mul (mat vec)
  "Multiply a matrix MAT by a column VEC"
  (aops:each-index i
      (aops:reduce-index #'+ j
          (* (aref mat i j) (aref vec j)))))

(defun linear-system-cost (mat v u)
  "Calculate a cost function for a system of linear equations MAT*X=V at the
  point X=U"
  (let ((val (matrix-vector-mul mat u)))
    (reduce
     #'+ (map 'vector
              (lambda (v1 v2)
                (expt (- v1 v2) 2))
              v val))))

(defun %make-array (list shape)
  (make-array shape
              :element-type     'real
              :initial-contents list))

(defun my-system (u)
  (linear-system-cost
   (linear-system:%make-array
    '((1 2 1 3)
      (2 1 1 1)
      (1 3 2 4)
      (1 5 1 1))
    '(4 4))
   (linear-system:%make-array
    '(18 12 28 23)
    '(4))
   u))

(defun my-system-grad (u)
  (diff:ad-multivariate #'my-system u))
      @end(code)

      Now evaluate the following code in the REPL:
      @begin[lang=lisp](code)
(cl-optim:bfgs/magicl
 #'linear-system:my-system #'linear-system:my-system-grad
 (magicl:zeros '(4)))
      @end(code)

      The answer is
      @begin[lang=lisp](code)
#<MAGICL:VECTOR/DOUBLE-FLOAT (4):
   1.000
   3.000
   5.000
   2.000>
#<MAGICL:MATRIX/DOUBLE-FLOAT (4x4):
  14.000    24.000    12.000    20.000
  24.000    77.993    28.000    48.002
  12.000    28.000    14.000    26.000
  20.000    48.002    26.000    53.998>
7
      @end(code)
      The first value is the minimizer of @c(my-system): \((1, 3, 5, 2)\). The
      second value is Hessian of the cost function at \(x\). The last value is a
      number of iterations needed to minimize the cost function.
   @end(section)
@end(section)
