<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  API &ndash; cl-optim
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <script src="static/load-mathjax.js" async></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="api"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="api"] {
       font-weight: bold;
   }

   .toc li a[data-node="api"] + ol {
       display: block;
   }

   .toc li a[data-node="api"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-optim</h1>
  <article id="article" data-section="api">
    <aside>
      <ol class="toc"><li><a href="index.html" data-node="index">Manual</a><ol><li><a href="index.html#gradient-based-algorithms" data-node="gradient-based-algorithms">Gradient-based algorithms</a></li><li><a href="index.html#algorithms-which-work-without-gradient" data-node="algorithms-which-work-without-gradient">Algorithms which work without gradient</a></li><li><a href="index.html#linear-least-squares" data-node="linear-least-squares">Linear least squares</a></li></ol></li><li><a href="api.html" data-node="api">API</a></li><li><a href="tips.html" data-node="tips">Tips</a></li><li><a href="more-examples.html" data-node="more-examples">More examples</a><ol><li><a href="solving-a-system-of-linear-equations.html" data-node="solving-a-system-of-linear-equations">Solving a system of linear equations</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">API</h2>
      </header>
      <div class="content">
        
   <u>Gradient-based algorithms</u>
   
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">gradient-descent</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using vanilla gradient descent.<code>function</code> takes a vector of values of type <code>cl-forward-diff:dual</code>
and returns one value of type <code>cl-forward-diff:dual</code>(see
documentation for cl-forward-diff). <code>start-point</code> must be a vector
of double floats and serves as a starting point for the
algorithm. <code>η</code> controls how fast the algorithm follows the gradient
of the function.</p><p>The algorithm stops when either a number of iterations exceeds
<code>max-iterations</code> or absolute value of every gradient component is
less than <code>ε</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">gradient-descent-momentum</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using gradient descent with momentum.<code>β1</code> controls how fast the momentum decreases on a flat surface. For
a description of other parameters, see <code>gradient-descent</code>.</p><p>The algorithm stops when either a number of iterations exceeds
<code>max-iterations</code> or absolute value of every gradient component and
momentum component is less than <code>ε</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">nag</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using Nesterov's advanced gradient
descent algorithm. For a description of parameters see
<code>gradient-descent</code> and <code>gradient-descent-momentum</code>.</p><p>Exit criterion is the same as in <code>gradient-descent-momentum</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">adam</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (β2 *β2*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using Adam. <code>β2</code> is a parameter used
in calculation of the second momentum of the gradient. For a
description of other parameters see <code>gradient-descent</code> and
<code>gradient-descent-momentum</code>.</p><p>Exit criterion is the same as in <code>gradient-descent-momentum</code>.</p></div></div>
   
   <u>Gradient-based algorithms with Hessian approximation</u>
   
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">bfgs</code><code class="codex-lambda-list">(function initial-approximation &amp;key (backtracking-options *default-backtracking-options*) (ε 1.0d-6) (max-steps 10000))</code><div class="codex-docstring">BFGS variant which works with native Common Lisp arrays. See
<code>bfgs/magicl</code> for detailed documentation.</div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">bfgs/magicl</code><code class="codex-lambda-list">(function initial-approximation &amp;key (backtracking-options *default-backtracking-options*) (ε 1.0d-6) (max-steps 10000))</code><div class="codex-docstring">Minimize a function using BFGS with backtracking line search
algorithm. <code>Function</code> is a differentiable function to be minimized
and <code>initial-approximation</code> is a MAGICL vector which contains a
starting point for the search. The search stops when either the number
of steps exceeds <code>max-steps</code> or L²-norm of the gradient is below
<code>ε</code>. This function returns a vector which minimizes <code>function</code>, an
approximation of Hessian at this point and a total number of steps.</div></div>
      <div class="codex-error codex-no-node">No node with name <code>backtracking-options</code>.</div>
   
   <u>Least squares fit</u>
   
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">linear-least-squares</code><code class="codex-lambda-list">(ys &amp;rest xs)</code><div class="codex-docstring"><p>Find \(\beta\) which minimizes the following expression:</p><p>\(|(Y - f(X_0, X_1, \dots))|^2\)</p><p>where \(f\) is</p><p>\(f(X_0, X_1, \dots) = X_0 \beta_0 + X_1 \beta_1 + \dots + \beta_N \)
(\(N\) being the last element in the returned array).</p><p>Also the value of the target expression is returned as the second value.</p><p>All passed arrays must be of the same size. This function may signal
SIMPLE-ERROR if they are too short.</p></div></div>
   
   <u>Heuristics</u>
   
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">simulated-annealing</code><code class="codex-lambda-list">(function start-point &amp;key (max-iterations most-positive-fixnum) (initial-temperature 1.0d0) (final-temperature *final-temperature*) (cooldown (exponential-cooldown 0.999d0)) (next (normal-neighborhood 1.0d0)))</code><div class="codex-docstring"><p>Find a minimum of <code>function</code> using simulated annealing
algorithm. The function must take a vector of <code>double-float</code> numbers
and return a <code>double-float</code> number, in other words this method does
not require the function to be differentiable. <code>start-point</code> is a
vector of <code>double-float</code> numbers which serves as a starting point for
a search. The algorithm exists when either the number of iterations
exceeds <code>max-iterations</code> or the temperature drops below
<code>final-temperature</code>.<code>cooldown</code> may be used to explicitly specify a cooldown
schedule. The cooldown schedule must be a function which takes the
current temperature and value of <code>function</code> at the current point and
returns new temperature.<code>next</code> is a function which takes a point in optimization space (as a
vector of double float numbers) and returns a new point as a new
candidate for a minimum.</p><p>This function returns the found minimum and an object of type
<code>simulated-annealing-summary</code> which contains some statistics like a
total number of iterations, the final temperature and so on.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">exponential-cooldown</code><code class="codex-lambda-list">(λ)</code><div class="codex-docstring">Create exponential cooldown schedule which multiplies the
temperature by a parameter <code>0 &lt; λ &lt; 1</code></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">normal-neighborhood</code><code class="codex-lambda-list">(σ)</code><div class="codex-docstring">Create a neighborhood function which calculates a new candidate by
adding a vector of independent random values with distribution
<code>N(0, σ)</code> to the current candidate.</div></div>
   
   <u>Parameters</u>
   
      <div class="codex-doc-node codex-variable"><code class="codex-name">*ε*</code><div class="codex-docstring">Exit criterion for gradient descent algorithms.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*η*</code><div class="codex-docstring">Default descent rate for gradient descent algorithms.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*β1*</code><div class="codex-docstring">Default parameter for calculating the first momentum of the
gradient in algorithms with momentum.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*β2*</code><div class="codex-docstring">Default parameter for calculating the second momentum of the
gradient in Adam.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*max-iterations*</code><div class="codex-docstring">Maximal allowed number of iterations.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*final-temperature*</code><div class="codex-docstring">Final temperature for the simulated annealing method</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*default-backtracking-options*</code><div class="codex-docstring">Default options for the backtracking line search algorithm.</div></div>
   

      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
