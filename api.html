<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  API &ndash; cl-optim
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="api"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="api"] {
       font-weight: bold;
   }

   .toc li a[data-node="api"] + ol {
       display: block;
   }

   .toc li a[data-node="api"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-optim</h1>
  <article id="article" data-section="api">
    <aside>
      <ol class="toc"><li><a href="index.html" data-node="index">Manual</a><ol><li><a href="index.html#gradient-based-algorithms" data-node="gradient-based-algorithms">Gradient-based algorithms</a></li></ol></li><li><a href="api.html" data-node="api">API</a></li><li><a href="tips.html" data-node="tips">Tips</a></li><li><a href="more-examples.html" data-node="more-examples">More examples</a><ol><li><a href="solving-a-system-of-linear-equations.html" data-node="solving-a-system-of-linear-equations">Solving a system of linear equations</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">API</h2>
      </header>
      <div class="content">
        
   <u>Algorithms</u>
   
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">gradient-descent</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using vanilla gradient descent.<code>function</code> takes a list of values of type <code>cl-forward-diff:dual</code>
and returns one value of type <code>cl-forward-diff:dual</code>(see
documentation for cl-forward-diff). <code>start-point</code> must be a list of
single floats and serves as a starting point for the
algorithm. <code>η</code> controls how fast the algorithm follows the gradient
of the function.</p><p>The algorithms exists when either a number of iterations exceeds
<code>max-iterations</code> or absolute value of every gradient component is
less than <code>ε</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">gradient-descent-momentum</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using gradient descent with momentum.<code>β1</code> controls how fast the momentum decreases on a flat surface. For
a description of other parameters, see <code>gradient-descent</code>.</p><p>The algorithms exists when either a number of iterations exceeds
<code>max-iterations</code> or absolute value of every gradient component and
momentum component is less than <code>ε</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">nag</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using Nesterov's advanced gradient
descent algorithm. For a description of parameters see
<code>gradient-descent</code> and <code>gradient-descent-momentum</code>.</p><p>Exit criterion is the same as in <code>gradient-descent-momentum</code>.</p></div></div>
      <div class="codex-doc-node codex-operator codex-function"><code class="codex-name">adam</code><code class="codex-lambda-list">(function start-point &amp;key (η *η*) (ε *ε*) (β1 *β1*) (β2 *β2*) (max-iterations *max-iterations*))</code><div class="codex-docstring"><p>Find minimum of a function using Adam. <code>β2</code> is a parameter used
in calculation of the second momentum of the gradient. For a
description of other parameters see <code>gradient-descent</code> and
<code>gradient-descent-momentum</code>.</p><p>Exit criterion is the same as in <code>gradient-descent-momentum</code>.</p></div></div>
   
   <u>Parameters</u>
   
      <div class="codex-doc-node codex-variable"><code class="codex-name">*ε*</code><div class="codex-docstring">Exit criterion for gradient descent algorithms.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*η*</code><div class="codex-docstring">Default descent rate for gradient descent algorithms.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*β1*</code><div class="codex-docstring">Default parameter for calculating the first momentum of the
gradient in algorithms with momentum.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*β2*</code><div class="codex-docstring">Default parameter for calculating the second momentum of the
gradient in Adam.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*max-iterations*</code><div class="codex-docstring">Maximal allowed number of iterations.</div></div>
   

      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
