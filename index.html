<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  Manual &ndash; cl-optim
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <script src="static/load-mathjax.js" async></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="index"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="index"] {
       font-weight: bold;
   }

   .toc li a[data-node="index"] + ol {
       display: block;
   }

   .toc li a[data-node="index"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-optim</h1>
  <article id="article" data-section="index">
    <aside>
      <ol class="toc"><li><a href="index.html" data-node="index">Manual</a><ol><li><a href="index.html#gradient-based-algorithms" data-node="gradient-based-algorithms">Gradient-based algorithms</a></li><li><a href="index.html#algorithms-which-work-without-gradient" data-node="algorithms-which-work-without-gradient">Algorithms which work without gradient</a></li><li><a href="index.html#linear-least-squares" data-node="linear-least-squares">Linear least squares</a></li></ol></li><li><a href="api.html" data-node="api">API</a></li><li><a href="tips.html" data-node="tips">Tips</a></li><li><a href="more-examples.html" data-node="more-examples">More examples</a><ol><li><a href="solving-a-system-of-linear-equations.html" data-node="solving-a-system-of-linear-equations">Solving a system of linear equations</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">Manual</h2>
      </header>
      <div class="content">
        <p>
   CL-OPTIM system provides a collection of algorithms for minimizing real
   functions of one or many arguments.</p><p>   </p><h1 id="gradient-based-algorithms">Gradient-based algorithms</h1><p>
      These algorithms require the function to be differentiable at almost every
      point. There is no need to define the derivative of the function you wish
      to minimize. A special system,
      <a href="https://github.com/shamazmazum/cl-forward-diff">cl-forward-diff</a>,
      is capable of automatic differentiation. For example, let's write a
      function which evaluates a polynomial at point <code>x</code> given the polynomial
      coefficients <code>coeffs</code>. For the sake of automatic differentiation we must
      not use standard mathematical functions from the <code>cl</code> package. Instead
      we must use functions from <code>cl-forward-diff</code> package. If you mix them
      you will get either runtime error or compile time error (depending on your
      implementation). You will never get a wrong result, so don't worry about
      accidentally mixing these functions. To minimize a probability of an error
      I recommend to define differentiable functions in a separate package which
      shadows mathematical functions from <code>cl</code> with ones provided by
      <code>cl-forward-diff</code>. Our function, <code>polynomial</code> can look like this:
      </p><pre><code class="lisp">(defpackage polynomial
  (:use #:cl #:snakes)
  (:import-from #:cl-forward-diff #:dual)
  (:import-from #:serapeum #:-&gt;)
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:polynomial))
(in-package :polynomial)

(-&gt; polynomial (list dual)(values dual &amp;optional))
(defun polynomial (coeffs x)
  (declare (type dual x))
  (reduce-generator
   #'+
   (imap (lambda (c n)
           (* c (expt x n)))
         (list-&gt;generator coeffs)
         (icount 0))))
      </code></pre><p>
      Type declarations are added for clarity, but combining them with
      declarations like <code>(declare (optimize (speed 3)))</code> can also result in
      better generated code.</p><p>      As you can see, differentiable functions operate not with usual numbers
      but with values of type <code>dual</code>(so called dual numbers). You can
      evaluate some polynomial, let it be \(f(x) = 1 + 2x + 3x^2\) at the
      point \(x\)(let \(x = 2\)) along with its first derivative \(f'(x) = 2 +
      6x\) like this:
      </p><pre><code class="repl">CL-USER&gt; (polynomial:polynomial '(1 2 3) #d(2d0 1d0))
#&lt;SIMD-PACK 1.7000000000000d+1 1.4000000000000d+1&gt;
      </code></pre><p>
      The type <code>dual</code> is implemented as a structure which has two slots for
      double float numbers, so all calculations are performed with double
      precision.</p><p>      Now when we have this function, <code>polynomial</code>, we want to find the
      minimum of our polynomial f(x). We can use the simplest gradient descent
      algorithm for this purpose. Try the following code in the REPL:
      </p><pre><code class="repl">CL-USER&gt; (cl-optim:gradient-descent
 (alexandria:compose (alexandria:curry #'polynomial:polynomial '(1 2 3))
                     (alexandria:rcurry #'aref 0))
 (cl-forward-diff:to-doubles '(10)))
#(-0.3331667786236352d0)
3673
      </code></pre><p>
      The first argument of <code>gradient-descent</code> is a function to be
      minimized. It takes a vector of dual numbers (let its length be <code>n</code>) and
      return a dual number. We must compose partially applied <code>polynomial</code>
      with <code>(lambda (xs)(aref xs 0))</code> because <code>polynomial</code> excepts a single
      dual number, not a vector of duals. The second argument is a starting
      point for a search. It is a vector of double floats of the same length
      <code>n</code>. Because our function is convex, just every starting point will do.</p><p>      The first returned value is the found minimum and the second is the number
      of steps required to find that minimum. You can use a better algorithm to
      reduce the number of steps:
      </p><pre><code class="repl">CL-USER&gt; (cl-optim:nag
          (alexandria:compose (alexandria:curry #'polynomial:polynomial '(1 2 3))
                     (alexandria:rcurry #'aref 0))
          '(10))
#(-0.33344855804382767d0)
139
      </code></pre><p>      Just another example of minimizing a function of two variables (Rosenbrock
      function with parameters <code>a = 2</code> and <code>b = 100</code>).
      </p><pre><code class="lisp">(defpackage rosenbrock
  (:use #:cl)
  (:import-from #:cl-forward-diff #:dual)
  #.(cl-forward-diff:shadowing-import-math)
  (:export #:rosenbrock))
(in-package :rosenbrock)

(defun rosenbrock (xs)
  (let ((x (aref xs 0))
        (y (aref xs 1)))
    (+ (expt (- 2 x) 2)
       (* 100 (expt (- y (expt x 2)) 2)))))
      </code></pre><p>
      Evaluate in the REPL:
      </p><pre><code class="repl">CL-USER&gt; (cl-optim:nag #'rosenbrock:rosenbrock
                       (cl-forward-diff:to-doubles '(-1 1))
                       :η  1d-4
                       :ε  1d-5
                       :β1 0.99d0)
#(1.999993247190274d0 3.9999729695129296d0)
5275
      </code></pre><p>
      Here I overrided some default parameters <code>β1</code>, <code>ε</code> and <code>η</code>. For more
      information see <a id="api" href="api.html" data-node="api">API</a> section.
   </p><p>   </p><h1 id="algorithms-which-work-without-gradient">Algorithms which work without gradient</h1><p>      Currently there is only one algorithm in <code>cl-optim</code> which can optimize
      non-differentiable functions: simulated annealing. The function you wish
      to minimize must operate with <code>double-float</code> numbers, not with
      <code>dual</code>s. Take this function as an example.</p><p>      </p><pre><code class="lisp">(defun noise-sinc (x)
  (declare (type double-float x))
  ;; Here all math function are from CL package.
  (let ((x-shift (- x 4)))
    (- (random 2d-1)
       (if (zerop x-shift) 1d0
           (/ (sin (* 5 x-shift)) x-shift)))))
      </code></pre><p>      This is a <i>sinc</i> function subtracted from a random noise with a small
      amplitude. This function can not be differentiated due to this noise. Now
      try this in the REPL:</p><p>      </p><pre><code class="repl">CL-USER&gt; (cl-optim:simulated-annealing
          (alexandria:compose #'noise-sinc
                              (alexandria:rcurry #'aref 0))
          (cl-forward-diff:to-doubles '(15)))
#(4.0656966038580755d0)
#S(CL-OPTIM:SIMULATED-ANNEALING-SUMMARY
   :TEMPERATURE 9.99734336301017d-5
   :ITERATIONS 27764
   :MINIMIZING-STEPS 8247
   :REJECTED-STEPS 18558)
      </code></pre><p>      The minimum is around the point <code>(4d0)</code>. As you can see, this method
      does a big amount of evaluations of the function, but it does not require
      the function to be differentiable.
   </p><p>   </p><h1 id="linear-least-squares">Linear least squares</h1><p>
   You can solve linear least squares fit problem with <code>cl-optim</code>. Suppose you
   have a vector of arguments of some function (x'es) and function of values
   (y's). You can approximate that function with a linear function like so:
</p><pre><code class="lisp">CL-USER&gt; (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 3))
          (cl-forward-diff:to-doubles '(1 2 4)))
#(0.6428571428571426d0 0.4999999999999998d0)
0.07142857142857147d0
</code></pre><p>   Your linear approximation is \(f(x) = 0.64 x + 0.5\). The second value is
   the sum of squared differences between \(f(x)\) and supplied \(y\). When
   a number of samples is 2, the second value is zero:
</p><pre><code class="lisp">CL-USER&gt; (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 3))
          (cl-forward-diff:to-doubles '(1 5)))
#(0.5d0 0.5d0)
0.0d0
</code></pre><p>
   Indeed, \(1 \cdot \frac{1}{2} + \frac{1}{2} = 1\) and \(5 \cdot \frac{1}{2} +
   \frac{1}{2} = 3\).</p><p>   Multivariate problem can also be solved:
</p><pre><code class="lisp">CL-USER&gt; (cl-optim:linear-least-squares
          (cl-forward-diff:to-doubles '(1 2 6))
          (cl-forward-diff:to-doubles '(1 2 2))
          (cl-forward-diff:to-doubles '(1 2 1)))
#(4.9999999999999964d0 -3.999999999999991d0 -1.0658141036401503d-14)
1.072850831100576d-28
</code></pre><p>
   which corresponds to the function \(f(x_0, x_1) = 5 x_0 - 4 x_1\). Indeed, we
   have \(5 \cdot 1 - 4 \cdot 1 = 1\), \(5 \cdot 2 - 4 \cdot 2 = 2\) and \(5
   \cdot 2 - 4 \cdot 1 = 6\).
   </p><p>
</p>
      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
